# NanoGPT-From-Scratch

A minimal implementation of GPT architecture from scratch in PyTorch, designed to replicate the performance of GPT-2 with a clean, modular, and educational codebase.

---

## ðŸš€ Project Overview

This project is a ground-up implementation of a GPT-style transformer model, built to:
- Closely match the performance of GPT-2 on benchmark tasks
- Serve as an educational resource for understanding transformers
- Allow flexible experimentation with model size, tokenizer, dataset, and training schedules

Inspired by Karpathyâ€™s NanoGPT.